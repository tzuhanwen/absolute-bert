{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359223f1-d317-43a1-be68-4b773357ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9868a467-07ba-4294-98a9-0494580542db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessor import Stopwords_preprocessor\n",
    "from utils.markdown import beir_metrics_to_markdown_table\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# from rank_bm25 import BM25Okapi as BM25\n",
    "from transformers import logging, AutoTokenizer\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5b9f58-6074-4391-a283-1c57c4a2c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cace617d-0c06-41a1-a705-d5e118a859de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d3331f-6e34-4319-b419-1049b7525e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data\\trec-covid.zip: 100%|███████████████████████████████████████████████████████| 70.5M/70.5M [00:31<00:00, 2.33MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from beir import util\n",
    "dataset =  'trec-covid' # \"nfcorpus\" \n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "data_path = util.download_and_unzip(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a191d-035e-4a54-8512-ff756a1c44e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecedc775-7d5d-46d8-ada5-51dc9f93fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 171332/171332 [00:01<00:00, 140897.88it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'scifact'\n",
    "corpus_name = 'trec-covid'\n",
    "# corpus_name = 'nfcorpus'\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(f'data/{corpus_name}').load(split=\"test\")\n",
    "corpus_text = [v['text'] for k,v in corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74415317-b92a-41ba-857d-a6fc32f1e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 23s\n",
      "Wall time: 2min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;function tokenize at 0x000001F4C0297F60&gt;,\n",
       "                vocabulary={&#x27;!&#x27;: 999, &#x27;&quot;&#x27;: 1000, &#x27;#&#x27;: 1001, &#x27;##!&#x27;: 29612,\n",
       "                            &#x27;##&quot;&#x27;: 29613, &#x27;###&#x27;: 29614, &#x27;##$&#x27;: 29615,\n",
       "                            &#x27;##%&#x27;: 29616, &#x27;##&amp;&#x27;: 29617, &quot;##&#x27;&quot;: 29618,\n",
       "                            &#x27;##(&#x27;: 29619, &#x27;##)&#x27;: 29620, &#x27;##*&#x27;: 29621,\n",
       "                            &#x27;##+&#x27;: 29622, &#x27;##,&#x27;: 29623, &#x27;##-&#x27;: 29624,\n",
       "                            &#x27;##.&#x27;: 29625, &#x27;##/&#x27;: 29626, &#x27;##0&#x27;: 2692,\n",
       "                            &#x27;##00&#x27;: 8889, &#x27;##01&#x27;: 24096, &#x27;##0s&#x27;: 16223,\n",
       "                            &#x27;##1&#x27;: 2487, &#x27;##10&#x27;: 10790, &#x27;##100&#x27;: 18613,\n",
       "                            &#x27;##11&#x27;: 14526, &#x27;##12&#x27;: 12521, &#x27;##13&#x27;: 17134,\n",
       "                            &#x27;##14&#x27;: 16932, &#x27;##15&#x27;: 16068, ...})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(tokenizer=&lt;function tokenize at 0x000001F4C0297F60&gt;,\n",
       "                vocabulary={&#x27;!&#x27;: 999, &#x27;&quot;&#x27;: 1000, &#x27;#&#x27;: 1001, &#x27;##!&#x27;: 29612,\n",
       "                            &#x27;##&quot;&#x27;: 29613, &#x27;###&#x27;: 29614, &#x27;##$&#x27;: 29615,\n",
       "                            &#x27;##%&#x27;: 29616, &#x27;##&amp;&#x27;: 29617, &quot;##&#x27;&quot;: 29618,\n",
       "                            &#x27;##(&#x27;: 29619, &#x27;##)&#x27;: 29620, &#x27;##*&#x27;: 29621,\n",
       "                            &#x27;##+&#x27;: 29622, &#x27;##,&#x27;: 29623, &#x27;##-&#x27;: 29624,\n",
       "                            &#x27;##.&#x27;: 29625, &#x27;##/&#x27;: 29626, &#x27;##0&#x27;: 2692,\n",
       "                            &#x27;##00&#x27;: 8889, &#x27;##01&#x27;: 24096, &#x27;##0s&#x27;: 16223,\n",
       "                            &#x27;##1&#x27;: 2487, &#x27;##10&#x27;: 10790, &#x27;##100&#x27;: 18613,\n",
       "                            &#x27;##11&#x27;: 14526, &#x27;##12&#x27;: 12521, &#x27;##13&#x27;: 17134,\n",
       "                            &#x27;##14&#x27;: 16932, &#x27;##15&#x27;: 16068, ...})</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function tokenize at 0x000001F4C0297F60>,\n",
       "                vocabulary={'!': 999, '\"': 1000, '#': 1001, '##!': 29612,\n",
       "                            '##\"': 29613, '###': 29614, '##$': 29615,\n",
       "                            '##%': 29616, '##&': 29617, \"##'\": 29618,\n",
       "                            '##(': 29619, '##)': 29620, '##*': 29621,\n",
       "                            '##+': 29622, '##,': 29623, '##-': 29624,\n",
       "                            '##.': 29625, '##/': 29626, '##0': 2692,\n",
       "                            '##00': 8889, '##01': 24096, '##0s': 16223,\n",
       "                            '##1': 2487, '##10': 10790, '##100': 18613,\n",
       "                            '##11': 14526, '##12': 12521, '##13': 17134,\n",
       "                            '##14': 16932, '##15': 16068, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer.convert_ids_to_tokens(tokenizer.encode(x, add_special_tokens=False))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, vocabulary=tokenizer.vocab)\n",
    "%time vectorizer.fit(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f24ae7b-219e-4628-b1d1-1b7b8b0a1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# text_sample = corpus[list(corpus.keys())[0]]['text']\n",
    "# res = mean_rotary_discrepency(text_sample)\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8afab6a5-e82d-4247-824b-06d117a37f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(text):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(ids) == 0:\n",
    "        return np.zeros(word_reprs.shape[1])\n",
    "    return word_reprs[ids].mean(axis=0)\n",
    "\n",
    "def sum_vector(text):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return word_reprs[ids].sum(axis=0)\n",
    "\n",
    "\n",
    "def idf_mean_vector(text):\n",
    "  ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  # return (vectorizer.idf_[ids] @ word_reprs[ids]) / (len(ids) + 1e-8) # 這個比較慢，可能跟 contiguous 有關\n",
    "  return np.einsum('ld,l', word_reprs[ids], vectorizer.idf_[ids]) / (len(ids) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae78eab6-213d-42cb-8c83-bdb7656e4aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\abs_bert\\\\20240729-16-38-43\\\\batch_325000-model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m subpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20240729-16-38-43\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# abs_bert, 768dim\u001b[39;00m\n\u001b[0;32m      5\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m325000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m RNN \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43msubpath\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model = torch.load(folder_path/subpath/f'last_batch-model.pt', map_location='cpu')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m epoch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\abs_bert\\\\20240729-16-38-43\\\\batch_325000-model.pt'"
     ]
    }
   ],
   "source": [
    "folder_path = pathlib.Path('data/abs_bert/')\n",
    "\n",
    "subpath = '20240729-16-38-43'  # abs_bert, 768dim\n",
    "\n",
    "batch_num = 325000\n",
    "RNN = torch.load(folder_path/subpath/f'batch_{batch_num}-model.pt', map_location='cpu')\n",
    "# model = torch.load(folder_path/subpath/f'last_batch-model.pt', map_location='cpu')\n",
    "\n",
    "epoch_num = 0\n",
    "RNN = torch.load(folder_path/subpath/f'epoch_{epoch_num}-model.pt', map_location='cpu')\n",
    "\n",
    "RNN.eval()\n",
    "\n",
    "model = RNN.model\n",
    "word_reprs_complex = model.predictor.all_word_embeddings()\n",
    "word_reprs = torch.concat([word_reprs_complex.real, word_reprs_complex.imag], dim=-1).detach().numpy()\n",
    "word_reprs_complex = word_reprs_complex.detach().numpy()\n",
    "\n",
    "# angle_reprs = model.angle_embedding.weight.detach().numpy()\n",
    "\n",
    "\n",
    "vectors = word_reprs\n",
    "# vectors = angle_reprs\n",
    "\n",
    "_, (ax_w, ax_b) = plt.subplots(1, 2, figsize=[9, 4])\n",
    "\n",
    "# weight stats\n",
    "norms = np.linalg.norm(vectors, axis=1)\n",
    "indices = norms.argsort()\n",
    "print(f\"weight norms, min: {norms.min()}, max: {norms.max()}\")\n",
    "print(\"min norms:\", tokenizer.convert_ids_to_tokens(indices[:10]))\n",
    "print(\"max norms:\", tokenizer.convert_ids_to_tokens(indices[-10:]))\n",
    "_ = ax_w.hist(norms, bins=100)\n",
    "\n",
    "# bias stats\n",
    "biases = RNN.bias.detach()\n",
    "indices = biases.argsort()\n",
    "print(f\"bias norms, min: {biases.min()}, max: {biases.max()}\")\n",
    "print(\"min norms:\", tokenizer.convert_ids_to_tokens(indices[:10]))\n",
    "print(\"max norms:\", tokenizer.convert_ids_to_tokens(indices[-10:]))\n",
    "if hasattr(model.predictor, 'base_vec'):\n",
    "  print(f\"base vec norm: {model.predictor.base_vec.norm()}\")\n",
    "_ = ax_b.hist(biases, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1841,
   "id": "d2e4d477-966b-4a6d-8399-0d465f563a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer('I love you.', return_tensors='pt', add_special_tokens=False)\n",
    "# histories = [his.expand(4, *[-1]*(len(his.shape)-1)) for his in model.initial_states()['histories']]\n",
    "# output, next_histories = model.forward(inputs['input_ids'][0], 1, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1842,
   "id": "1aec96e1-6362-47df-a6e8-7bba46b04a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.45 s, sys: 424 ms, total: 6.87 s\n",
      "Wall time: 6.87 s\n",
      "CPU times: user 50 ms, sys: 0 ns, total: 50 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "method = idf_mean_vector\n",
    "method = mean_vector\n",
    "# method = sum_vector\n",
    "\n",
    "part = 'text'\n",
    "# part = 'title'\n",
    "\n",
    "%time text_vec_dict = OrderedDict({k: method(v[part]) for k, v in corpus.items()})\n",
    "%time query_vec_dict = OrderedDict({k: method(v) for k, v in queries.items()})\n",
    "text_vecs = np.stack(list(text_vec_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "id": "f10e4e1b-5b47-4be7-8231-a18caf29ca39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 1min 36s, total: 2min 3s\n",
      "Wall time: 2.47 s\n",
      "NDCG@1\tNDCG@3\tNDCG@5\tNDCG@10\tNDCG@100\tNDCG@1000\tMAP@1\tMAP@3\tMAP@5\tMAP@10\tMAP@100\tMAP@1000\tRecall@1\tRecall@3\tRecall@5\tRecall@10\tRecall@100\tRecall@1000\tP@1\tP@3\tP@5\tP@10\tP@100\tP@1000\n",
      "0.05667\t0.06835\t0.06979\t0.07919\t0.11019\t0.14457\t0.05667\t0.065\t0.06583\t0.06981\t0.07533\t0.07622\t0.05667\t0.07667\t0.08\t0.10778\t0.26028\t0.545\t0.05667\t0.02667\t0.01667\t0.01133\t0.00277\t0.00062\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "||NDCG|MAP|Recall|P|\n",
       "|-|-|-|-|-|\n",
       "|@1|0.0567|0.0567|0.0567|0.0567|\n",
       "|@3|0.0683|0.0650|0.0767|0.0267|\n",
       "|@5|0.0698|0.0658|0.0800|0.0167|\n",
       "|@10|0.0792|0.0698|0.1078|0.0113|\n",
       "|@100|0.1102|0.0753|0.2603|0.0028|\n",
       "|@1000|0.1446|0.0762|0.5450|0.0006|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'euclidean'\n",
    "metric = 'cosine'\n",
    "\n",
    "\n",
    "def score(query_vector, metric=metric):\n",
    "    return (1/pairwise_distances(query_vector[None, :], text_vecs, metric=metric))[0]\n",
    "\n",
    "%time results = {qid: dict(zip(text_vec_dict.keys(), score(query_vector).tolist())) \\\n",
    "            for qid, query_vector in query_vec_dict.items()}\n",
    "\n",
    "metrics = EvaluateRetrieval.evaluate(qrels, results, [1, 3, 5, 10, 100, 1000])\n",
    "\n",
    "flatten_metrics = {k: v for metric_type in metrics for k, v in metric_type.items()}\n",
    "metric_names, metric_values = zip(*flatten_metrics.items())\n",
    "print(*metric_names, sep='\\t')\n",
    "print(*metric_values, sep='\\t')\n",
    "print()\n",
    "\n",
    "md = beir_metrics_to_markdown_table(*metrics)\n",
    "Markdown(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85caf3-c5db-417f-9a62-9d56466133ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "id": "bb3aba8e-e9d5-4843-b467-fbad61c6578c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'histories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1737], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI love you.\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m histories \u001b[38;5;241m=\u001b[39m [his\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(his\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m his \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m      3\u001b[0m output, next_histories \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, histories)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'histories'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('I love you.', return_tensors='pt', add_special_tokens=False)\n",
    "histories = [his.expand(4, *[-1]*(len(his.shape)-1)) for his in model.initial_states()['histories']]\n",
    "output, next_histories = model.forward(inputs['input_ids'][0], 1, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "accbb11c-f59c-4cef-9330-d0fbc4c84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(model.limas)):\n",
    "#     lima_shape = model.limas[i].lima_shape\n",
    "#     print(lima_shape)\n",
    "#     print(f'{i}: {lima_shape.min()}, {lima_shape.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf9539e-9188-412d-ab27-e3b763375132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.6350, requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictor.rotary_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdecec59-c1d6-4131-b37e-e78e0b3bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write first 10 questions and top 10 answer to file\n",
    "\n",
    "# samples = list(results.items())[:10]\n",
    "# for q_num, score_dict in samples:\n",
    "#     with open(f'question_{q_num}.txt', 'w') as f:\n",
    "#         f.write(f'{queries[q_num]}\\n\\n')\n",
    "#         tokens = tokenizer.convert_ids_to_tokens(tokenizer(queries[q_num], add_special_tokens=False)['input_ids'])\n",
    "#         f.write(f'{tokens}\\n\\n')\n",
    "        \n",
    "#         text_ids, text_scores = zip(*score_dict.items())\n",
    "#         text_scores = np.array(text_scores)\n",
    "#         top_10_idx = np.argsort(text_scores)[:-10:-1]\n",
    "\n",
    "#         for idx in top_10_idx:\n",
    "#             f.write(f'{corpus[text_ids[idx]]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f43406fa-1299-46a9-8462-4398095cc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test: 看每個字往時間方向逆向轉一個 t 後，附近的字為何。理論來說會是跟這個字無關的字 (text independent)，因為這個旋轉抵銷了時間旋轉\n",
    "\n",
    "# inverse_metric_theta = - 1/model.predictor.rotary_denom**(model.predictor.dimension_nums/model.predictor.dim)\n",
    "# inverse_pos_rotation = torch.complex(inverse_metric_theta.cos(), inverse_metric_theta.sin())\n",
    "# least_effective_position_of_the_word = model.predictor.all_word_embeddings() * inverse_pos_rotation\n",
    "# least_effective_position_of_the_word = torch.concat([least_effective_position_of_the_word.real, least_effective_position_of_the_word.imag], dim=-1).detach().numpy()\n",
    "\n",
    "# least_effective_position_of_the_word.shape\n",
    "\n",
    "# %time d = pairwise_distances(word_reprs, least_effective_position_of_the_word, metric='euclidean') # metric='cosine'\n",
    "\n",
    "# %time pair = d.argsort(axis=1)[:, :10]\n",
    "\n",
    "# for input_id in tokenizer.encode(text_sample):\n",
    "#     print(f'{tokenizer.convert_ids_to_tokens(input_id)}: {tokenizer.convert_ids_to_tokens(pair[input_id])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
